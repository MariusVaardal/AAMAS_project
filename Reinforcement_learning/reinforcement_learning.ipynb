{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pettingzoo.mpe import simple_tag_v3\n",
    "from pettingzoo import ParallelEnv\n",
    "import numpy as np\n",
    "import functools\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('/home/mariusvaardal/AAMAS_project/AAMAS_project')\n",
    "\n",
    "import supersuit as ss\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_GOOD = 1\n",
    "NUM_ADV = 4\n",
    "NUM_OBST = 0\n",
    "MAX_CYCLES = 200\n",
    "CONTINOUS_ACTIONS = False\n",
    "RENDER_MODE = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_agent_0_from_dicts(dicts):\n",
    "    ret = []\n",
    "    for dict in dicts:\n",
    "        del dict['agent_0']\n",
    "        ret.append(dict)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'agent_types'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01magent_types\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mAvoidingAgent\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AvoidingAgent\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# from agent_types.AvoidingNearestAdversaryAgent import AvoidingNearestAdversaryAgent\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# from agent_types.ImmobileAgent import ImmobileAgent\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mCustomEnvironment\u001b[39;00m(ParallelEnv):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'agent_types'"
     ]
    }
   ],
   "source": [
    "from agent_types.AvoidingAgent import AvoidingAgent\n",
    "# from agent_types.AvoidingNearestAdversaryAgent import AvoidingNearestAdversaryAgent\n",
    "# from agent_types.ImmobileAgent import ImmobileAgent\n",
    "\n",
    "class CustomEnvironment(ParallelEnv):\n",
    "    metadata = {\n",
    "        \"name\": \"custom_environment_v0\",\n",
    "    }\n",
    "\n",
    "    def __init__(self, num_good, num_adversaries, num_obstacles, max_cycles, continuous_actions, render_mode):\n",
    "        self.env = simple_tag_v3.parallel_env(num_good=num_good, num_adversaries=num_adversaries, num_obstacles=num_obstacles, max_cycles=max_cycles, continuous_actions=continuous_actions, render_mode=render_mode)\n",
    "        self.env.reset() \n",
    "        # Setting all the required attributes\n",
    "        self.agents = [agent for agent in self.env.agents if agent.startswith(\"adversary\")]\n",
    "        self.possible_agents = [adv for adv in self.env.possible_agents if adv.startswith(\"adversary\")]\n",
    "        self.render_mode = render_mode\n",
    "        # Adding agent_0 as part of the environment. Agent_0 is not meant to be included in the training\n",
    "        # self.agent_0 = AvoidingNearestAdversaryAgent('agent_0', num_adversaries=NUM_ADV, num_landmarks=NUM_OBST)\n",
    "        self.agent_0 = AvoidingAgent('agent_0', num_adversaries=NUM_ADV, num_landmarks=NUM_OBST)\n",
    "        \n",
    "    def reset(self, seed=None, options=None):\n",
    "        observations, infos = self.env.reset(seed=seed, options=options)\n",
    "        self.agent_0.see(observations[self.agent_0.name])\n",
    "        observations, infos = remove_agent_0_from_dicts([observations, infos])\n",
    "        return observations, infos\n",
    "\n",
    "    def step(self, actions):\n",
    "        actions['agent_0'] = self.agent_0.get_action()\n",
    "        observations, rewards, terminations, truncations, infos =  self.env.step(actions)\n",
    "        if observations:\n",
    "            self.agent_0.see(observations[self.agent_0.name])\n",
    "            observations, rewards, terminations, truncations, infos = remove_agent_0_from_dicts([observations, rewards, terminations, truncations, infos])\n",
    "        return observations, rewards, terminations, truncations, infos\n",
    "\n",
    "    def render(self):\n",
    "        self.env.render()\n",
    "\n",
    "    # Observation space should be defined here.\n",
    "    # lru_cache allows observation and action spaces to be memoized, reducing clock cycles required to get each agent's space.\n",
    "    # If your spaces change over time, remove this line (disable caching).\n",
    "    @functools.lru_cache(maxsize=None)\n",
    "    def observation_space(self, agent):\n",
    "        return self.env.observation_space(agent)\n",
    "\n",
    "    # Action space should be defined here.\n",
    "    # If your spaces change over time, remove this line (disable caching).\n",
    "    @functools.lru_cache(maxsize=None)\n",
    "    def action_space(self, agent):\n",
    "        return self.env.action_space(agent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CustomEnvironment(num_good=NUM_GOOD, num_adversaries=NUM_ADV, num_obstacles=NUM_OBST, max_cycles=MAX_CYCLES, continuous_actions=CONTINOUS_ACTIONS, render_mode='human')\n",
    "observations, infos = env.reset()\n",
    "\n",
    "terminated = False\n",
    "timestep = 1\n",
    "while not terminated:\n",
    "    # this is where you would insert your policy\n",
    "    actions = {agent: env.action_space(agent).sample() for agent in env.agents}\n",
    "\n",
    "    observations, rewards, terminations, truncations, infos = env.step(actions)\n",
    "    if not observations:\n",
    "        terminated = True\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CustomEnvironment(num_good=NUM_GOOD, num_adversaries=NUM_ADV, num_obstacles=NUM_OBST, max_cycles=MAX_CYCLES, continuous_actions=CONTINOUS_ACTIONS, render_mode=RENDER_MODE)\n",
    "env.reset(seed=45)\n",
    "conv_env = ss.pettingzoo_env_to_vec_env_v1(env)\n",
    "conv_env = ss.concat_vec_envs_v1(conv_env, 2, num_cpus=0, base_class=\"stable_baselines3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "log_path = \"./logs/4_adv_0_to_50M_steps\"\n",
    "\n",
    "model = PPO(\n",
    "        MlpPolicy,\n",
    "        conv_env,\n",
    "        verbose=3,\n",
    "        learning_rate=1e-3,\n",
    "        batch_size=256,\n",
    "        tensorboard_log=log_path,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"./models/3_adv\"\n",
    "model = PPO.load(os.path.join(model_path, \"106M_ANAA\"), conv_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"./models/3_adv\"\n",
    "model = PPO.load(os.path.join(model_path, \"best_model\"), conv_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_object = {'learning_rate': 1e-4}\n",
    "\n",
    "model = PPO.load('./models/4_adv/best_model_30M/best_model', conv_env, custom_objects=custom_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "model = PPO(\n",
    "        MlpPolicy,\n",
    "        conv_env,\n",
    "        verbose=3,\n",
    "        learning_rate=1e-3,\n",
    "        batch_size=256,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 4772  |\n",
      "|    iterations      | 1     |\n",
      "|    time_elapsed    | 3     |\n",
      "|    total_timesteps | 16384 |\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "model.learn(total_timesteps=1_000)\n",
    "model.save(\"./models/4_adv/4_adv_1k_steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./logs/4_adv_0_to_50M_steps/PPO_7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 4383  |\n",
      "|    iterations      | 1     |\n",
      "|    time_elapsed    | 3     |\n",
      "|    total_timesteps | 16384 |\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x7eff5ec730d0>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stable_baselines3.common.callbacks import CallbackList, CheckpointCallback, EvalCallback\n",
    "\n",
    "best_model_save_path = \"./models/4_adv/\"\n",
    "\n",
    "# # Save a checkpoint every 1000 steps\n",
    "# checkpoint_callback = CheckpointCallback(\n",
    "#                              save_freq=500_000,\n",
    "#                              save_path=\"./models/4_adv\",\n",
    "#                              name_prefix=\"4_adv\",\n",
    "#                             )\n",
    "# eval_callback = EvalCallback(conv_env,\n",
    "#                              eval_freq=10_000,\n",
    "#                              best_model_save_path=best_model_save_path,\n",
    "#                              log_path=log_path,\n",
    "#                              n_eval_episodes=100,\n",
    "#                              verbose=1)\n",
    "\n",
    "# callback = CallbackList([eval_callback])\n",
    "\n",
    "# first 10M timesteps\n",
    "model.learn(total_timesteps=1000)\n",
    "model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m\n",
      "\u001b[0mPPO\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBufferedIOBase\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0menv\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgymnasium\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEnv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mForwardRef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'VecEnv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'auto'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mcustom_objects\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mprint_system_info\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mforce_reset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m~\u001b[0m\u001b[0mSelfBaseAlgorithm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSource:\u001b[0m   \n",
      "    \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m  \u001b[0;31m# noqa: C901\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mcls\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mType\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mSelfBaseAlgorithm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBufferedIOBase\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0menv\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mGymEnv\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"auto\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mcustom_objects\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mprint_system_info\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mforce_reset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mSelfBaseAlgorithm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"\u001b[0m\n",
      "\u001b[0;34m        Load the model from a zip-file.\u001b[0m\n",
      "\u001b[0;34m        Warning: ``load`` re-creates the model from scratch, it does not update it in-place!\u001b[0m\n",
      "\u001b[0;34m        For an in-place load use ``set_parameters`` instead.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        :param path: path to the file (or a file-like) where to\u001b[0m\n",
      "\u001b[0;34m            load the agent from\u001b[0m\n",
      "\u001b[0;34m        :param env: the new environment to run the loaded model on\u001b[0m\n",
      "\u001b[0;34m            (can be None if you only need prediction from a trained model) has priority over any saved environment\u001b[0m\n",
      "\u001b[0;34m        :param device: Device on which the code should run.\u001b[0m\n",
      "\u001b[0;34m        :param custom_objects: Dictionary of objects to replace\u001b[0m\n",
      "\u001b[0;34m            upon loading. If a variable is present in this dictionary as a\u001b[0m\n",
      "\u001b[0;34m            key, it will not be deserialized and the corresponding item\u001b[0m\n",
      "\u001b[0;34m            will be used instead. Similar to custom_objects in\u001b[0m\n",
      "\u001b[0;34m            ``keras.models.load_model``. Useful when you have an object in\u001b[0m\n",
      "\u001b[0;34m            file that can not be deserialized.\u001b[0m\n",
      "\u001b[0;34m        :param print_system_info: Whether to print system info from the saved model\u001b[0m\n",
      "\u001b[0;34m            and the current system info (useful to debug loading issues)\u001b[0m\n",
      "\u001b[0;34m        :param force_reset: Force call to ``reset()`` before training\u001b[0m\n",
      "\u001b[0;34m            to avoid unexpected behavior.\u001b[0m\n",
      "\u001b[0;34m            See https://github.com/DLR-RM/stable-baselines3/issues/597\u001b[0m\n",
      "\u001b[0;34m        :param kwargs: extra arguments to change the model when loading\u001b[0m\n",
      "\u001b[0;34m        :return: new model instance with loaded parameters\u001b[0m\n",
      "\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mprint_system_info\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"== CURRENT SYSTEM INFO ==\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mget_system_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpytorch_variables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_from_zip_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mprint_system_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprint_system_info\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32massert\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"No data found in the saved file\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32massert\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"No params found in the saved file\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# Remove stored device information and replace with ours\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0;34m\"policy_kwargs\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0;34m\"device\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"policy_kwargs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;32mdel\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"policy_kwargs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;31m# backward compatibility, convert to new format\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0;34m\"net_arch\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"policy_kwargs\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"policy_kwargs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"net_arch\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0msaved_net_arch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"policy_kwargs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"net_arch\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaved_net_arch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaved_net_arch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"policy_kwargs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"net_arch\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msaved_net_arch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0;34m\"policy_kwargs\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"policy_kwargs\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"policy_kwargs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;34mf\"The specified policy kwargs do not equal the stored policy kwargs.\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;34mf\"Stored kwargs: {data['policy_kwargs']}, specified kwargs: {kwargs['policy_kwargs']}\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0;34m\"observation_space\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"action_space\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The observation_space and action_space were not given, can't verify new environments\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# Gym -> Gymnasium space conversion\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"observation_space\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"action_space\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_convert_space\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0menv\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;31m# Wrap first if needed\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrap_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"verbose\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;31m# Check if given env is valid\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mcheck_for_correct_spaces\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"observation_space\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"action_space\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;31m# Discard `_last_obs`, this will force the env to reset before training\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;31m# See issue https://github.com/DLR-RM/stable-baselines3/issues/597\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0mforce_reset\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_last_obs\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;31m# `n_envs` must be updated. See issue https://github.com/DLR-RM/stable-baselines3/issues/1018\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"n_envs\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_envs\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;31m# Use stored env, if one exists. If not, continue as is (can be used for predict)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0;34m\"env\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"env\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mpolicy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"policy_class\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0menv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0m_init_setup_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# load parameters\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;31m# put state_dicts back in place\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexact_match\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;31m# Patch to load Policy saved using SB3 < 1.7.0\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;31m# the error is probably due to old policy being loaded\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;31m# See https://github.com/DLR-RM/stable-baselines3/issues/1233\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0;34m\"pi_features_extractor\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"Missing key(s) in state_dict\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexact_match\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0;34m\"You are probably loading a model saved with SB3 < 1.7.0, \"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0;34m\"we deactivated exact_match so you can save the model \"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0;34m\"again to avoid issues in the future \"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0;34m\"(see https://github.com/DLR-RM/stable-baselines3/issues/1233 for more info). \"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0;34mf\"Original error: {e} \\n\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0;34m\"Note: the model should still work fine, this only a warning.\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# put other pytorch variables back in place\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mpytorch_variables\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpytorch_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;31m# Skip if PyTorch variable was not defined (to ensure backward compatibility).\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;31m# This happens when using SAC/TQC.\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;31m# SAC has an entropy coefficient which can be fixed or optimized.\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;31m# If it is optimized, an additional PyTorch variable `log_ent_coef` is defined,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;31m# otherwise it is initialized to `None`.\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;32mif\u001b[0m \u001b[0mpytorch_variables\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;31m# Set the data attribute directly to avoid issue when using optimizers\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;31m# See https://github.com/DLR-RM/stable-baselines3/issues/391\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mrecursive_setattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"{name}.data\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpytorch_variables\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# Sample gSDE exploration matrix, so it uses the right device\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# see issue #44\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_sde\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_noise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[operator]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFile:\u001b[0m      ~/.local/lib/python3.10/site-packages/stable_baselines3/common/base_class.py\n",
      "\u001b[0;31mType:\u001b[0m      method"
     ]
    }
   ],
   "source": [
    "PPO.load??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(841.34, 95.57617067030881)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "evaluate_policy(model=model, env=conv_env, n_eval_episodes=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['adversary_0', 'adversary_1', 'adversary_2']\n",
      "Episode reward: 710.0\n"
     ]
    }
   ],
   "source": [
    "RENDER_MODE = None\n",
    "MAX_CYCLES = 200\n",
    "env = CustomEnvironment(num_good=NUM_GOOD, num_adversaries=NUM_ADV, num_obstacles=NUM_OBST, max_cycles=MAX_CYCLES, continuous_actions=CONTINOUS_ACTIONS, render_mode=RENDER_MODE)\n",
    "observations, infos = env.reset()\n",
    "\n",
    "terminated = False\n",
    "timestep = 1\n",
    "episode_reward = 0\n",
    "print(env.agents)\n",
    "while not terminated:\n",
    "\n",
    "    # this is where you would insert your policy\n",
    "    actions = {agent: model.predict(observations[agent])[0] for agent in env.agents}\n",
    "\n",
    "    observations, rewards, terminations, truncations, infos = env.step(actions)\n",
    "\n",
    "    episode_reward += sum(rewards.values()) / NUM_ADV\n",
    "\n",
    "    if not observations:\n",
    "        terminated = True\n",
    "    \n",
    "print(f\"Episode reward: {episode_reward}\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1:\n",
      "\n",
      "from pettingzoo.mpe import simple_tag_v3\n",
      "from pettingzoo import ParallelEnv\n",
      "import numpy as np\n",
      "import functools\n",
      "import os\n",
      "import sys\n",
      "sys.path.append('/home/mariusvaardal/AAMAS_project/AAMAS_project')\n",
      "\n",
      "import supersuit as ss\n",
      "from stable_baselines3 import PPO\n",
      "from stable_baselines3.ppo import MlpPolicy\n",
      "\n",
      "from stable_baselines3.common.evaluation import evaluate_policy\n",
      "   2:\n",
      "NUM_GOOD = 1\n",
      "NUM_ADV = 3\n",
      "NUM_OBST = 0\n",
      "MAX_CYCLES = 50\n",
      "CONTINOUS_ACTIONS = False\n",
      "   3:\n",
      "def remove_agent_0_from_dicts(dicts):\n",
      "    ret = []\n",
      "    for dict in dicts:\n",
      "        del dict['agent_0']\n",
      "        ret.append(dict)\n",
      "    return ret\n",
      "   4:\n",
      "# from agent_types.AvoidingAgent import AvoidingAgent\n",
      "# from agent_types.AvoidingNearestAdversaryAgent import AvoidingNearestAdversaryAgent\n",
      "from agent_types.ImmobileAgent import ImmobileAgent\n",
      "\n",
      "class CustomEnvironment(ParallelEnv):\n",
      "    metadata = {\n",
      "        \"name\": \"custom_environment_v0\",\n",
      "    }\n",
      "\n",
      "    def __init__(self, num_good, num_adversaries, num_obstacles, max_cycles, continuous_actions, render_mode):\n",
      "        self.env = simple_tag_v3.parallel_env(num_good=num_good, num_adversaries=num_adversaries, num_obstacles=num_obstacles, max_cycles=max_cycles, continuous_actions=continuous_actions, render_mode=render_mode)\n",
      "        self.env.reset() \n",
      "        # Setting all the required attributes\n",
      "        self.agents = [agent for agent in self.env.agents if agent.startswith(\"adversary\")]\n",
      "        self.possible_agents = [adv for adv in self.env.possible_agents if adv.startswith(\"adversary\")]\n",
      "        self.render_mode = render_mode\n",
      "        # Adding agent_0 as part of the environment. Agent_0 is not meant to be included in the training\n",
      "        self.agent_0 = ImmobileAgent('agent_0', num_adversaries=NUM_ADV, num_landmarks=NUM_OBST)\n",
      "        \n",
      "    def reset(self, seed=None, options=None):\n",
      "        observations, infos = self.env.reset(seed=seed, options=options)\n",
      "        self.agent_0.see(observations[self.agent_0.name])\n",
      "        observations, infos = remove_agent_0_from_dicts([observations, infos])\n",
      "        return observations, infos\n",
      "\n",
      "    def step(self, actions):\n",
      "        actions['agent_0'] = self.agent_0.get_action()\n",
      "        observations, rewards, terminations, truncations, infos =  self.env.step(actions)\n",
      "        if observations:\n",
      "            self.agent_0.see(observations[self.agent_0.name])\n",
      "            observations, rewards, terminations, truncations, infos = remove_agent_0_from_dicts([observations, rewards, terminations, truncations, infos])\n",
      "        return observations, rewards, terminations, truncations, infos\n",
      "\n",
      "    def render(self):\n",
      "        self.env.render()\n",
      "\n",
      "    # Observation space should be defined here.\n",
      "    # lru_cache allows observation and action spaces to be memoized, reducing clock cycles required to get each agent's space.\n",
      "    # If your spaces change over time, remove this line (disable caching).\n",
      "    @functools.lru_cache(maxsize=None)\n",
      "    def observation_space(self, agent):\n",
      "        return self.env.observation_space(agent)\n",
      "\n",
      "    # Action space should be defined here.\n",
      "    # If your spaces change over time, remove this line (disable caching).\n",
      "    @functools.lru_cache(maxsize=None)\n",
      "    def action_space(self, agent):\n",
      "        return self.env.action_space(agent)\n",
      "   5:\n",
      "env = CustomEnvironment(num_good=NUM_GOOD, num_adversaries=NUM_ADV, num_obstacles=NUM_OBST, max_cycles=MAX_CYCLES, continuous_actions=CONTINOUS_ACTIONS, render_mode=RENDER_MODE)\n",
      "env.reset(seed=45)\n",
      "conv_env = ss.pettingzoo_env_to_vec_env_v1(env)\n",
      "conv_env = ss.concat_vec_envs_v1(conv_env, 2, num_cpus=1, base_class=\"stable_baselines3\")\n",
      "   6:\n",
      "NUM_GOOD = 1\n",
      "NUM_ADV = 3\n",
      "NUM_OBST = 0\n",
      "MAX_CYCLES = 50\n",
      "CONTINOUS_ACTIONS = False\n",
      "RENDER_MODE = None\n",
      "   7:\n",
      "env = CustomEnvironment(num_good=NUM_GOOD, num_adversaries=NUM_ADV, num_obstacles=NUM_OBST, max_cycles=MAX_CYCLES, continuous_actions=CONTINOUS_ACTIONS, render_mode=RENDER_MODE)\n",
      "env.reset(seed=45)\n",
      "conv_env = ss.pettingzoo_env_to_vec_env_v1(env)\n",
      "conv_env = ss.concat_vec_envs_v1(conv_env, 2, num_cpus=1, base_class=\"stable_baselines3\")\n",
      "   8:\n",
      "model = PPO(\n",
      "        MlpPolicy,\n",
      "        conv_env,\n",
      "        verbose=3,\n",
      "        learning_rate=1e-3,\n",
      "        batch_size=256,\n",
      "    )\n",
      "   9:\n",
      "model_save_path = \"../models\"\n",
      "model.learn(total_timesteps=1000)\n",
      "model.save(model_save_path)\n",
      "  10:\n",
      "model_save_path = \"../../models\"\n",
      "model.learn(total_timesteps=1000)\n",
      "model.save(model_save_path)\n",
      "  11:\n",
      "model_save_path = \"../../models/model\"\n",
      "model.learn(total_timesteps=1000)\n",
      "model.save(model_save_path)\n",
      "  12:\n",
      "\n",
      "from pettingzoo.mpe import simple_tag_v3\n",
      "from pettingzoo import ParallelEnv\n",
      "import numpy as np\n",
      "import functools\n",
      "import os\n",
      "import sys\n",
      "sys.path.append('/home/mariusvaardal/AAMAS_project/AAMAS_project')\n",
      "\n",
      "import supersuit as ss\n",
      "from stable_baselines3 import PPO\n",
      "from stable_baselines3.ppo import MlpPolicy\n",
      "\n",
      "from stable_baselines3.common.evaluation import evaluate_policy\n",
      "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\n",
      "  13:\n",
      "best_model_save_path = \"../../best_model/best_model\"\n",
      "log_path = \"../../logs/log\"\n",
      "threshold_callback = StopTrainingOnRewardThreshold(reward_threshold=300, verbose=1)\n",
      "eval_callback = EvalCallback(env,\n",
      "                             eval_freq=10_000,\n",
      "                             best_model_save_path=best_model_save_path,\n",
      "                             callback_on_new_best=threshold_callback,\n",
      "                             log_path=log_path,\n",
      "                             n_eval_episodes=10,\n",
      "                             verbose=1)\n",
      "model.learn(total_timesteps=5_000_000, callback=eval_callback)\n",
      "  14:\n",
      "\n",
      "from pettingzoo.mpe import simple_tag_v3\n",
      "from pettingzoo import ParallelEnv\n",
      "import numpy as np\n",
      "import functools\n",
      "import os\n",
      "import sys\n",
      "sys.path.append('/home/mariusvaardal/AAMAS_project/AAMAS_project')\n",
      "\n",
      "import supersuit as ss\n",
      "from stable_baselines3 import PPO\n",
      "from stable_baselines3.ppo import MlpPolicy\n",
      "\n",
      "from stable_baselines3.common.evaluation import evaluate_policy\n",
      "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\n",
      "import shimmy\n",
      "  15:\n",
      "best_model_save_path = \"../../best_model/best_model\"\n",
      "log_path = \"../../logs/log\"\n",
      "threshold_callback = StopTrainingOnRewardThreshold(reward_threshold=300, verbose=1)\n",
      "eval_callback = EvalCallback(env,\n",
      "                             eval_freq=10_000,\n",
      "                             best_model_save_path=best_model_save_path,\n",
      "                             callback_on_new_best=threshold_callback,\n",
      "                             log_path=log_path,\n",
      "                             n_eval_episodes=10,\n",
      "                             verbose=1)\n",
      "model.learn(total_timesteps=5_000_000, callback=eval_callback)\n",
      "  16:\n",
      "best_model_save_path = \"../../best_model/best_model\"\n",
      "log_path = \"../../logs/log\"\n",
      "threshold_callback = StopTrainingOnRewardThreshold(reward_threshold=300, verbose=1)\n",
      "eval_callback = EvalCallback(conv_env,\n",
      "                             eval_freq=10_000,\n",
      "                             best_model_save_path=best_model_save_path,\n",
      "                             callback_on_new_best=threshold_callback,\n",
      "                             log_path=log_path,\n",
      "                             n_eval_episodes=10,\n",
      "                             verbose=1)\n",
      "model.learn(total_timesteps=5_000_000, callback=eval_callback)\n",
      "  17: model.learn(total_timesteps=1000)\n",
      "  18:\n",
      "env = CustomEnvironment(num_good=NUM_GOOD, num_adversaries=NUM_ADV, num_obstacles=NUM_OBST, max_cycles=MAX_CYCLES, continuous_actions=CONTINOUS_ACTIONS, render_mode=RENDER_MODE)\n",
      "env.reset(seed=45)\n",
      "conv_env = ss.pettingzoo_env_to_vec_env_v1(env)\n",
      "conv_env = ss.concat_vec_envs_v1(conv_env, 0, num_cpus=1, base_class=\"stable_baselines3\")\n",
      "  19:\n",
      "env = CustomEnvironment(num_good=NUM_GOOD, num_adversaries=NUM_ADV, num_obstacles=NUM_OBST, max_cycles=MAX_CYCLES, continuous_actions=CONTINOUS_ACTIONS, render_mode=RENDER_MODE)\n",
      "env.reset(seed=45)\n",
      "conv_env = ss.pettingzoo_env_to_vec_env_v1(env)\n",
      "conv_env = ss.concat_vec_envs_v1(conv_env, 2, num_cpus=0, base_class=\"stable_baselines3\")\n",
      "  20:\n",
      "model = PPO(\n",
      "        MlpPolicy,\n",
      "        conv_env,\n",
      "        verbose=3,\n",
      "        learning_rate=1e-3,\n",
      "        batch_size=256,\n",
      "    )\n"
     ]
    }
   ],
   "source": [
    "%history -n 1-20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from agent_types.AvoidingAgent import AvoidingAgent\n",
    "# from agent_types.AvoidingNearestAdversaryAgent import AvoidingNearestAdversaryAgent\n",
    "from agent_types.ImmobileAgent import ImmobileAgent\n",
    "\n",
    "class CustomEnvironment(ParallelEnv):\n",
    "    metadata = {\n",
    "        \"name\": \"custom_environment_v0\",\n",
    "    }\n",
    "\n",
    "    def __init__(self, num_good, num_adversaries, num_obstacles, max_cycles, continuous_actions, render_mode):\n",
    "        self.env = simple_tag_v3.parallel_env(num_good=num_good, num_adversaries=num_adversaries, num_obstacles=num_obstacles, max_cycles=max_cycles, continuous_actions=continuous_actions, render_mode=render_mode)\n",
    "        self.env.reset() \n",
    "        # Setting all the required attributes\n",
    "        self.agents = [agent for agent in self.env.agents if agent.startswith(\"adversary\")]\n",
    "        self.possible_agents = [adv for adv in self.env.possible_agents if adv.startswith(\"adversary\")]\n",
    "        self.render_mode = render_mode\n",
    "        # Adding agent_0 as part of the environment. Agent_0 is not meant to be included in the training\n",
    "        self.agent_0 = ImmobileAgent('agent_0', num_adversaries=NUM_ADV, num_landmarks=NUM_OBST)\n",
    "        \n",
    "    def reset(self, seed=None, options=None):\n",
    "        observations, infos = self.env.reset(seed=seed, options=options)\n",
    "        self.agent_0.see(observations[self.agent_0.name])\n",
    "        observations, infos = remove_agent_0_from_dicts([observations, infos])\n",
    "        return observations, infos\n",
    "\n",
    "    def step(self, actions):\n",
    "        actions['agent_0'] = self.agent_0.get_action()\n",
    "        observations, rewards, terminations, truncations, infos =  self.env.step(actions)\n",
    "        if observations:\n",
    "            self.agent_0.see(observations[self.agent_0.name])\n",
    "            observations, rewards, terminations, truncations, infos = remove_agent_0_from_dicts([observations, rewards, terminations, truncations, infos])\n",
    "        return observations, rewards, terminations, truncations, infos\n",
    "\n",
    "    def render(self):\n",
    "        self.env.render()\n",
    "\n",
    "    # Observation space should be defined here.\n",
    "    # lru_cache allows observation and action spaces to be memoized, reducing clock cycles required to get each agent's space.\n",
    "    # If your spaces change over time, remove this line (disable caching).\n",
    "    @functools.lru_cache(maxsize=None)\n",
    "    def observation_space(self, agent):\n",
    "        return self.env.observation_space(agent)\n",
    "\n",
    "    # Action space should be defined here.\n",
    "    # If your spaces change over time, remove this line (disable caching).\n",
    "    @functools.lru_cache(maxsize=None)\n",
    "    def action_space(self, agent):\n",
    "        return self.env.action_space(agent)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
