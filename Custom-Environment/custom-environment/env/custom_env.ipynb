{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pettingzoo.mpe import simple_tag_v3\n",
    "from pettingzoo import ParallelEnv\n",
    "import numpy as np\n",
    "import functools\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('/home/mariusvaardal/AAMAS_project/AAMAS_project')\n",
    "\n",
    "import supersuit as ss\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_GOOD = 1\n",
    "NUM_ADV = 3\n",
    "NUM_OBST = 0\n",
    "MAX_CYCLES = 50\n",
    "CONTINOUS_ACTIONS = False\n",
    "RENDER_MODE = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_agent_0_from_dicts(dicts):\n",
    "    ret = []\n",
    "    for dict in dicts:\n",
    "        del dict['agent_0']\n",
    "        ret.append(dict)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from agent_types.AvoidingAgent import AvoidingAgent\n",
    "# from agent_types.AvoidingNearestAdversaryAgent import AvoidingNearestAdversaryAgent\n",
    "from agent_types.ImmobileAgent import ImmobileAgent\n",
    "\n",
    "class CustomEnvironment(ParallelEnv):\n",
    "    metadata = {\n",
    "        \"name\": \"custom_environment_v0\",\n",
    "    }\n",
    "\n",
    "    def __init__(self, num_good, num_adversaries, num_obstacles, max_cycles, continuous_actions, render_mode):\n",
    "        self.env = simple_tag_v3.parallel_env(num_good=num_good, num_adversaries=num_adversaries, num_obstacles=num_obstacles, max_cycles=max_cycles, continuous_actions=continuous_actions, render_mode=render_mode)\n",
    "        self.env.reset() \n",
    "        # Setting all the required attributes\n",
    "        self.agents = [agent for agent in self.env.agents if agent.startswith(\"adversary\")]\n",
    "        self.possible_agents = [adv for adv in self.env.possible_agents if adv.startswith(\"adversary\")]\n",
    "        self.render_mode = render_mode\n",
    "        # Adding agent_0 as part of the environment. Agent_0 is not meant to be included in the training\n",
    "        self.agent_0 = ImmobileAgent('agent_0', num_adversaries=NUM_ADV, num_landmarks=NUM_OBST)\n",
    "        \n",
    "    def reset(self, seed=None, options=None):\n",
    "        observations, infos = self.env.reset(seed=seed, options=options)\n",
    "        self.agent_0.see(observations[self.agent_0.name])\n",
    "        observations, infos = remove_agent_0_from_dicts([observations, infos])\n",
    "        return observations, infos\n",
    "\n",
    "    def step(self, actions):\n",
    "        actions['agent_0'] = self.agent_0.get_action()\n",
    "        observations, rewards, terminations, truncations, infos =  self.env.step(actions)\n",
    "        if observations:\n",
    "            self.agent_0.see(observations[self.agent_0.name])\n",
    "            observations, rewards, terminations, truncations, infos = remove_agent_0_from_dicts([observations, rewards, terminations, truncations, infos])\n",
    "        return observations, rewards, terminations, truncations, infos\n",
    "\n",
    "    def render(self):\n",
    "        self.env.render()\n",
    "\n",
    "    # Observation space should be defined here.\n",
    "    # lru_cache allows observation and action spaces to be memoized, reducing clock cycles required to get each agent's space.\n",
    "    # If your spaces change over time, remove this line (disable caching).\n",
    "    @functools.lru_cache(maxsize=None)\n",
    "    def observation_space(self, agent):\n",
    "        return self.env.observation_space(agent)\n",
    "\n",
    "    # Action space should be defined here.\n",
    "    # If your spaces change over time, remove this line (disable caching).\n",
    "    @functools.lru_cache(maxsize=None)\n",
    "    def action_space(self, agent):\n",
    "        return self.env.action_space(agent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path = \"../../logs/log\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CustomEnvironment(num_good=NUM_GOOD, num_adversaries=NUM_ADV, num_obstacles=NUM_OBST, max_cycles=MAX_CYCLES, continuous_actions=CONTINOUS_ACTIONS, render_mode='human')\n",
    "observations, infos = env.reset()\n",
    "\n",
    "terminated = False\n",
    "timestep = 1\n",
    "while not terminated:\n",
    "    # this is where you would insert your policy\n",
    "    actions = {agent: env.action_space(agent).sample() for agent in env.agents}\n",
    "\n",
    "    observations, rewards, terminations, truncations, infos = env.step(actions)\n",
    "    if not observations:\n",
    "        terminated = True\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CustomEnvironment(num_good=NUM_GOOD, num_adversaries=NUM_ADV, num_obstacles=NUM_OBST, max_cycles=MAX_CYCLES, continuous_actions=CONTINOUS_ACTIONS, render_mode=RENDER_MODE)\n",
    "env.reset(seed=45)\n",
    "conv_env = ss.pettingzoo_env_to_vec_env_v1(env)\n",
    "conv_env = ss.concat_vec_envs_v1(conv_env, 2, num_cpus=0, base_class=\"stable_baselines3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "log_path = \"../../logs/log\"\n",
    "\n",
    "model = PPO(\n",
    "        MlpPolicy,\n",
    "        conv_env,\n",
    "        verbose=3,\n",
    "        learning_rate=1e-3,\n",
    "        batch_size=256,\n",
    "        tensorboard_log=log_path,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ../../logs/log/PPO_1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 4386  |\n",
      "|    iterations      | 1     |\n",
      "|    time_elapsed    | 2     |\n",
      "|    total_timesteps | 12288 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 3762        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009332046 |\n",
      "|    clip_fraction        | 0.102       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.6        |\n",
      "|    explained_variance   | -0.00683    |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 6.18        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00498    |\n",
      "|    value_loss           | 13.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 3586        |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009217011 |\n",
      "|    clip_fraction        | 0.084       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.59       |\n",
      "|    explained_variance   | 0.00384     |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 6.48        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00432    |\n",
      "|    value_loss           | 17.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 3424        |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 14          |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010276195 |\n",
      "|    clip_fraction        | 0.117       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.58       |\n",
      "|    explained_variance   | 0.00963     |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 3.15        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00391    |\n",
      "|    value_loss           | 15          |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=6.00 +/- 9.17\n",
      "Episode length: 50.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 50           |\n",
      "|    mean_reward          | 6            |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 60000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0093987705 |\n",
      "|    clip_fraction        | 0.129        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.57        |\n",
      "|    explained_variance   | 0.153        |\n",
      "|    learning_rate        | 0.001        |\n",
      "|    loss                 | 4.14         |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00452     |\n",
      "|    value_loss           | 10.9         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mariusvaardal/.local/lib/python3.10/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 3360  |\n",
      "|    iterations      | 5     |\n",
      "|    time_elapsed    | 18    |\n",
      "|    total_timesteps | 61440 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 3326         |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 22           |\n",
      "|    total_timesteps      | 73728        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074867937 |\n",
      "|    clip_fraction        | 0.0768       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.55        |\n",
      "|    explained_variance   | 0.154        |\n",
      "|    learning_rate        | 0.001        |\n",
      "|    loss                 | 15           |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.00412     |\n",
      "|    value_loss           | 31           |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 3300        |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 86016       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007863901 |\n",
      "|    clip_fraction        | 0.0847      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.53       |\n",
      "|    explained_variance   | 0.191       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 7.93        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.00443    |\n",
      "|    value_loss           | 25.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 3288        |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 98304       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010877271 |\n",
      "|    clip_fraction        | 0.122       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.52       |\n",
      "|    explained_variance   | 0.135       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 10.3        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00648    |\n",
      "|    value_loss           | 24.1        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 3259       |\n",
      "|    iterations           | 9          |\n",
      "|    time_elapsed         | 33         |\n",
      "|    total_timesteps      | 110592     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00820996 |\n",
      "|    clip_fraction        | 0.0854     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.51      |\n",
      "|    explained_variance   | 0.214      |\n",
      "|    learning_rate        | 0.001      |\n",
      "|    loss                 | 13.1       |\n",
      "|    n_updates            | 80         |\n",
      "|    policy_gradient_loss | -0.00545   |\n",
      "|    value_loss           | 29.8       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=120000, episode_reward=36.00 +/- 19.60\n",
      "Episode length: 50.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 50          |\n",
      "|    mean_reward          | 36          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 120000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010405929 |\n",
      "|    clip_fraction        | 0.12        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.47       |\n",
      "|    explained_variance   | 0.293       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 17.4        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00788    |\n",
      "|    value_loss           | 48.4        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 3242   |\n",
      "|    iterations      | 10     |\n",
      "|    time_elapsed    | 37     |\n",
      "|    total_timesteps | 122880 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 3237        |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 41          |\n",
      "|    total_timesteps      | 135168      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009034487 |\n",
      "|    clip_fraction        | 0.0909      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.45       |\n",
      "|    explained_variance   | 0.217       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 36.4        |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.00674    |\n",
      "|    value_loss           | 96.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 3225        |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 45          |\n",
      "|    total_timesteps      | 147456      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008442581 |\n",
      "|    clip_fraction        | 0.0769      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | 0.162       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 58.5        |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.00684    |\n",
      "|    value_loss           | 118         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 3219        |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 49          |\n",
      "|    total_timesteps      | 159744      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009217429 |\n",
      "|    clip_fraction        | 0.0887      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.37       |\n",
      "|    explained_variance   | 0.276       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 56.8        |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.00742    |\n",
      "|    value_loss           | 130         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 3210        |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 53          |\n",
      "|    total_timesteps      | 172032      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008972491 |\n",
      "|    clip_fraction        | 0.085       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.34       |\n",
      "|    explained_variance   | 0.2         |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 88.5        |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.00779    |\n",
      "|    value_loss           | 183         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=180000, episode_reward=149.00 +/- 24.68\n",
      "Episode length: 50.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 50          |\n",
      "|    mean_reward          | 149         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 180000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007925241 |\n",
      "|    clip_fraction        | 0.075       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.29       |\n",
      "|    explained_variance   | 0.341       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 57.4        |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.00644    |\n",
      "|    value_loss           | 152         |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 3180   |\n",
      "|    iterations      | 15     |\n",
      "|    time_elapsed    | 57     |\n",
      "|    total_timesteps | 184320 |\n",
      "-------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 10\u001b[0m\n\u001b[1;32m      2\u001b[0m threshold_callback \u001b[38;5;241m=\u001b[39m StopTrainingOnRewardThreshold(reward_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      3\u001b[0m eval_callback \u001b[38;5;241m=\u001b[39m EvalCallback(conv_env,\n\u001b[1;32m      4\u001b[0m                              eval_freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10_000\u001b[39m,\n\u001b[1;32m      5\u001b[0m                              best_model_save_path\u001b[38;5;241m=\u001b[39mbest_model_save_path,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m                              n_eval_episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m      9\u001b[0m                              verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5_000_000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_callback\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py:315\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    307\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[1;32m    308\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    313\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    314\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[0;32m--> 315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:313\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mep_info_buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    311\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dump_logs(iteration)\n\u001b[0;32m--> 313\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    315\u001b[0m callback\u001b[38;5;241m.\u001b[39mon_training_end()\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py:217\u001b[0m, in \u001b[0;36mPPO.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_sde:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39mreset_noise(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size)\n\u001b[0;32m--> 217\u001b[0m values, log_prob, entropy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_actions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrollout_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m values \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m    219\u001b[0m \u001b[38;5;66;03m# Normalize advantage\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/stable_baselines3/common/policies.py:740\u001b[0m, in \u001b[0;36mActorCriticPolicy.evaluate_actions\u001b[0;34m(self, obs, actions)\u001b[0m\n\u001b[1;32m    738\u001b[0m log_prob \u001b[38;5;241m=\u001b[39m distribution\u001b[38;5;241m.\u001b[39mlog_prob(actions)\n\u001b[1;32m    739\u001b[0m values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_net(latent_vf)\n\u001b[0;32m--> 740\u001b[0m entropy \u001b[38;5;241m=\u001b[39m \u001b[43mdistribution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mentropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    741\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m values, log_prob, entropy\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/stable_baselines3/common/distributions.py:295\u001b[0m, in \u001b[0;36mCategoricalDistribution.entropy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mentropy\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m th\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 295\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mentropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/distributions/categorical.py:146\u001b[0m, in \u001b[0;36mCategorical.entropy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    144\u001b[0m min_real \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfinfo(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogits\u001b[38;5;241m.\u001b[39mdtype)\u001b[38;5;241m.\u001b[39mmin\n\u001b[1;32m    145\u001b[0m logits \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mclamp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogits, \u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39mmin_real)\n\u001b[0;32m--> 146\u001b[0m p_log_p \u001b[38;5;241m=\u001b[39m logits \u001b[38;5;241m*\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprobs\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39mp_log_p\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/distributions/utils.py:125\u001b[0m, in \u001b[0;36mlazy_property.__get__\u001b[0;34m(self, instance, obj_type)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m instance \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _lazy_property_and_property(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrapped)\n\u001b[0;32m--> 125\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39menable_grad():\n\u001b[1;32m    126\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrapped(instance)\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28msetattr\u001b[39m(instance, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrapped\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, value)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/grad_mode.py:134\u001b[0m, in \u001b[0;36menable_grad.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01menable_grad\u001b[39;00m(_NoParamDecoratorContextManager):\n\u001b[1;32m     88\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Context-manager that enables gradient calculation.\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \n\u001b[1;32m     90\u001b[0m \u001b[38;5;124;03m    Enables gradient calculation, if it has been disabled via :class:`~no_grad`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    131\u001b[0m \n\u001b[1;32m    132\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__enter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    135\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprev \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mis_grad_enabled()\n\u001b[1;32m    136\u001b[0m         torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_set_grad_enabled(\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_model_save_path = \"../../best_model/best_model\"\n",
    "threshold_callback = StopTrainingOnRewardThreshold(reward_threshold=300, verbose=1)\n",
    "eval_callback = EvalCallback(conv_env,\n",
    "                             eval_freq=10_000,\n",
    "                             best_model_save_path=best_model_save_path,\n",
    "                             callback_on_new_best=threshold_callback,\n",
    "                             log_path=log_path,\n",
    "                             n_eval_episodes=10,\n",
    "                             verbose=1)\n",
    "model.learn(total_timesteps=5_000_000, callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(167.0, 48.590122453025366)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_policy(model=model, env=conv_env, n_eval_episodes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "RENDER_MODE = \"human\"\n",
    "env = CustomEnvironment(num_good=NUM_GOOD, num_adversaries=NUM_ADV, num_obstacles=NUM_OBST, max_cycles=MAX_CYCLES, continuous_actions=CONTINOUS_ACTIONS, render_mode=RENDER_MODE)\n",
    "observations, infos = env.reset()\n",
    "\n",
    "terminated = False\n",
    "timestep = 1\n",
    "episode_reward = 0\n",
    "while not terminated:\n",
    "\n",
    "    # this is where you would insert your policy\n",
    "    actions = {agent: model.predict(observations[agent])[0] for agent in env.agents}\n",
    "\n",
    "    observations, rewards, terminations, truncations, infos = env.step(actions)\n",
    "\n",
    "    if not observations:\n",
    "        terminated = True\n",
    "    \n",
    "print(f\"Episode reward: {episode_reward}\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(2)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs = [1, 2, 2, 1,1, 2, 2, 1,1, 2, 2, 1]\n",
    "model.predict(obs)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1:\n",
      "\n",
      "from pettingzoo.mpe import simple_tag_v3\n",
      "from pettingzoo import ParallelEnv\n",
      "import numpy as np\n",
      "import functools\n",
      "import os\n",
      "import sys\n",
      "sys.path.append('/home/mariusvaardal/AAMAS_project/AAMAS_project')\n",
      "\n",
      "import supersuit as ss\n",
      "from stable_baselines3 import PPO\n",
      "from stable_baselines3.ppo import MlpPolicy\n",
      "\n",
      "from stable_baselines3.common.evaluation import evaluate_policy\n",
      "   2:\n",
      "NUM_GOOD = 1\n",
      "NUM_ADV = 3\n",
      "NUM_OBST = 0\n",
      "MAX_CYCLES = 50\n",
      "CONTINOUS_ACTIONS = False\n",
      "   3:\n",
      "def remove_agent_0_from_dicts(dicts):\n",
      "    ret = []\n",
      "    for dict in dicts:\n",
      "        del dict['agent_0']\n",
      "        ret.append(dict)\n",
      "    return ret\n",
      "   4:\n",
      "# from agent_types.AvoidingAgent import AvoidingAgent\n",
      "# from agent_types.AvoidingNearestAdversaryAgent import AvoidingNearestAdversaryAgent\n",
      "from agent_types.ImmobileAgent import ImmobileAgent\n",
      "\n",
      "class CustomEnvironment(ParallelEnv):\n",
      "    metadata = {\n",
      "        \"name\": \"custom_environment_v0\",\n",
      "    }\n",
      "\n",
      "    def __init__(self, num_good, num_adversaries, num_obstacles, max_cycles, continuous_actions, render_mode):\n",
      "        self.env = simple_tag_v3.parallel_env(num_good=num_good, num_adversaries=num_adversaries, num_obstacles=num_obstacles, max_cycles=max_cycles, continuous_actions=continuous_actions, render_mode=render_mode)\n",
      "        self.env.reset() \n",
      "        # Setting all the required attributes\n",
      "        self.agents = [agent for agent in self.env.agents if agent.startswith(\"adversary\")]\n",
      "        self.possible_agents = [adv for adv in self.env.possible_agents if adv.startswith(\"adversary\")]\n",
      "        self.render_mode = render_mode\n",
      "        # Adding agent_0 as part of the environment. Agent_0 is not meant to be included in the training\n",
      "        self.agent_0 = ImmobileAgent('agent_0', num_adversaries=NUM_ADV, num_landmarks=NUM_OBST)\n",
      "        \n",
      "    def reset(self, seed=None, options=None):\n",
      "        observations, infos = self.env.reset(seed=seed, options=options)\n",
      "        self.agent_0.see(observations[self.agent_0.name])\n",
      "        observations, infos = remove_agent_0_from_dicts([observations, infos])\n",
      "        return observations, infos\n",
      "\n",
      "    def step(self, actions):\n",
      "        actions['agent_0'] = self.agent_0.get_action()\n",
      "        observations, rewards, terminations, truncations, infos =  self.env.step(actions)\n",
      "        if observations:\n",
      "            self.agent_0.see(observations[self.agent_0.name])\n",
      "            observations, rewards, terminations, truncations, infos = remove_agent_0_from_dicts([observations, rewards, terminations, truncations, infos])\n",
      "        return observations, rewards, terminations, truncations, infos\n",
      "\n",
      "    def render(self):\n",
      "        self.env.render()\n",
      "\n",
      "    # Observation space should be defined here.\n",
      "    # lru_cache allows observation and action spaces to be memoized, reducing clock cycles required to get each agent's space.\n",
      "    # If your spaces change over time, remove this line (disable caching).\n",
      "    @functools.lru_cache(maxsize=None)\n",
      "    def observation_space(self, agent):\n",
      "        return self.env.observation_space(agent)\n",
      "\n",
      "    # Action space should be defined here.\n",
      "    # If your spaces change over time, remove this line (disable caching).\n",
      "    @functools.lru_cache(maxsize=None)\n",
      "    def action_space(self, agent):\n",
      "        return self.env.action_space(agent)\n",
      "   5:\n",
      "env = CustomEnvironment(num_good=NUM_GOOD, num_adversaries=NUM_ADV, num_obstacles=NUM_OBST, max_cycles=MAX_CYCLES, continuous_actions=CONTINOUS_ACTIONS, render_mode=RENDER_MODE)\n",
      "env.reset(seed=45)\n",
      "conv_env = ss.pettingzoo_env_to_vec_env_v1(env)\n",
      "conv_env = ss.concat_vec_envs_v1(conv_env, 2, num_cpus=1, base_class=\"stable_baselines3\")\n",
      "   6:\n",
      "NUM_GOOD = 1\n",
      "NUM_ADV = 3\n",
      "NUM_OBST = 0\n",
      "MAX_CYCLES = 50\n",
      "CONTINOUS_ACTIONS = False\n",
      "RENDER_MODE = None\n",
      "   7:\n",
      "env = CustomEnvironment(num_good=NUM_GOOD, num_adversaries=NUM_ADV, num_obstacles=NUM_OBST, max_cycles=MAX_CYCLES, continuous_actions=CONTINOUS_ACTIONS, render_mode=RENDER_MODE)\n",
      "env.reset(seed=45)\n",
      "conv_env = ss.pettingzoo_env_to_vec_env_v1(env)\n",
      "conv_env = ss.concat_vec_envs_v1(conv_env, 2, num_cpus=1, base_class=\"stable_baselines3\")\n",
      "   8:\n",
      "model = PPO(\n",
      "        MlpPolicy,\n",
      "        conv_env,\n",
      "        verbose=3,\n",
      "        learning_rate=1e-3,\n",
      "        batch_size=256,\n",
      "    )\n",
      "   9:\n",
      "model_save_path = \"../models\"\n",
      "model.learn(total_timesteps=1000)\n",
      "model.save(model_save_path)\n",
      "  10:\n",
      "model_save_path = \"../../models\"\n",
      "model.learn(total_timesteps=1000)\n",
      "model.save(model_save_path)\n",
      "  11:\n",
      "model_save_path = \"../../models/model\"\n",
      "model.learn(total_timesteps=1000)\n",
      "model.save(model_save_path)\n",
      "  12:\n",
      "\n",
      "from pettingzoo.mpe import simple_tag_v3\n",
      "from pettingzoo import ParallelEnv\n",
      "import numpy as np\n",
      "import functools\n",
      "import os\n",
      "import sys\n",
      "sys.path.append('/home/mariusvaardal/AAMAS_project/AAMAS_project')\n",
      "\n",
      "import supersuit as ss\n",
      "from stable_baselines3 import PPO\n",
      "from stable_baselines3.ppo import MlpPolicy\n",
      "\n",
      "from stable_baselines3.common.evaluation import evaluate_policy\n",
      "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\n",
      "  13:\n",
      "best_model_save_path = \"../../best_model/best_model\"\n",
      "log_path = \"../../logs/log\"\n",
      "threshold_callback = StopTrainingOnRewardThreshold(reward_threshold=300, verbose=1)\n",
      "eval_callback = EvalCallback(env,\n",
      "                             eval_freq=10_000,\n",
      "                             best_model_save_path=best_model_save_path,\n",
      "                             callback_on_new_best=threshold_callback,\n",
      "                             log_path=log_path,\n",
      "                             n_eval_episodes=10,\n",
      "                             verbose=1)\n",
      "model.learn(total_timesteps=5_000_000, callback=eval_callback)\n",
      "  14:\n",
      "\n",
      "from pettingzoo.mpe import simple_tag_v3\n",
      "from pettingzoo import ParallelEnv\n",
      "import numpy as np\n",
      "import functools\n",
      "import os\n",
      "import sys\n",
      "sys.path.append('/home/mariusvaardal/AAMAS_project/AAMAS_project')\n",
      "\n",
      "import supersuit as ss\n",
      "from stable_baselines3 import PPO\n",
      "from stable_baselines3.ppo import MlpPolicy\n",
      "\n",
      "from stable_baselines3.common.evaluation import evaluate_policy\n",
      "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\n",
      "import shimmy\n",
      "  15:\n",
      "best_model_save_path = \"../../best_model/best_model\"\n",
      "log_path = \"../../logs/log\"\n",
      "threshold_callback = StopTrainingOnRewardThreshold(reward_threshold=300, verbose=1)\n",
      "eval_callback = EvalCallback(env,\n",
      "                             eval_freq=10_000,\n",
      "                             best_model_save_path=best_model_save_path,\n",
      "                             callback_on_new_best=threshold_callback,\n",
      "                             log_path=log_path,\n",
      "                             n_eval_episodes=10,\n",
      "                             verbose=1)\n",
      "model.learn(total_timesteps=5_000_000, callback=eval_callback)\n",
      "  16:\n",
      "best_model_save_path = \"../../best_model/best_model\"\n",
      "log_path = \"../../logs/log\"\n",
      "threshold_callback = StopTrainingOnRewardThreshold(reward_threshold=300, verbose=1)\n",
      "eval_callback = EvalCallback(conv_env,\n",
      "                             eval_freq=10_000,\n",
      "                             best_model_save_path=best_model_save_path,\n",
      "                             callback_on_new_best=threshold_callback,\n",
      "                             log_path=log_path,\n",
      "                             n_eval_episodes=10,\n",
      "                             verbose=1)\n",
      "model.learn(total_timesteps=5_000_000, callback=eval_callback)\n",
      "  17: model.learn(total_timesteps=1000)\n",
      "  18:\n",
      "env = CustomEnvironment(num_good=NUM_GOOD, num_adversaries=NUM_ADV, num_obstacles=NUM_OBST, max_cycles=MAX_CYCLES, continuous_actions=CONTINOUS_ACTIONS, render_mode=RENDER_MODE)\n",
      "env.reset(seed=45)\n",
      "conv_env = ss.pettingzoo_env_to_vec_env_v1(env)\n",
      "conv_env = ss.concat_vec_envs_v1(conv_env, 0, num_cpus=1, base_class=\"stable_baselines3\")\n",
      "  19:\n",
      "env = CustomEnvironment(num_good=NUM_GOOD, num_adversaries=NUM_ADV, num_obstacles=NUM_OBST, max_cycles=MAX_CYCLES, continuous_actions=CONTINOUS_ACTIONS, render_mode=RENDER_MODE)\n",
      "env.reset(seed=45)\n",
      "conv_env = ss.pettingzoo_env_to_vec_env_v1(env)\n",
      "conv_env = ss.concat_vec_envs_v1(conv_env, 2, num_cpus=0, base_class=\"stable_baselines3\")\n",
      "  20:\n",
      "model = PPO(\n",
      "        MlpPolicy,\n",
      "        conv_env,\n",
      "        verbose=3,\n",
      "        learning_rate=1e-3,\n",
      "        batch_size=256,\n",
      "    )\n"
     ]
    }
   ],
   "source": [
    "%history -n 1-20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from agent_types.AvoidingAgent import AvoidingAgent\n",
    "# from agent_types.AvoidingNearestAdversaryAgent import AvoidingNearestAdversaryAgent\n",
    "from agent_types.ImmobileAgent import ImmobileAgent\n",
    "\n",
    "class CustomEnvironment(ParallelEnv):\n",
    "    metadata = {\n",
    "        \"name\": \"custom_environment_v0\",\n",
    "    }\n",
    "\n",
    "    def __init__(self, num_good, num_adversaries, num_obstacles, max_cycles, continuous_actions, render_mode):\n",
    "        self.env = simple_tag_v3.parallel_env(num_good=num_good, num_adversaries=num_adversaries, num_obstacles=num_obstacles, max_cycles=max_cycles, continuous_actions=continuous_actions, render_mode=render_mode)\n",
    "        self.env.reset() \n",
    "        # Setting all the required attributes\n",
    "        self.agents = [agent for agent in self.env.agents if agent.startswith(\"adversary\")]\n",
    "        self.possible_agents = [adv for adv in self.env.possible_agents if adv.startswith(\"adversary\")]\n",
    "        self.render_mode = render_mode\n",
    "        # Adding agent_0 as part of the environment. Agent_0 is not meant to be included in the training\n",
    "        self.agent_0 = ImmobileAgent('agent_0', num_adversaries=NUM_ADV, num_landmarks=NUM_OBST)\n",
    "        \n",
    "    def reset(self, seed=None, options=None):\n",
    "        observations, infos = self.env.reset(seed=seed, options=options)\n",
    "        self.agent_0.see(observations[self.agent_0.name])\n",
    "        observations, infos = remove_agent_0_from_dicts([observations, infos])\n",
    "        return observations, infos\n",
    "\n",
    "    def step(self, actions):\n",
    "        actions['agent_0'] = self.agent_0.get_action()\n",
    "        observations, rewards, terminations, truncations, infos =  self.env.step(actions)\n",
    "        if observations:\n",
    "            self.agent_0.see(observations[self.agent_0.name])\n",
    "            observations, rewards, terminations, truncations, infos = remove_agent_0_from_dicts([observations, rewards, terminations, truncations, infos])\n",
    "        return observations, rewards, terminations, truncations, infos\n",
    "\n",
    "    def render(self):\n",
    "        self.env.render()\n",
    "\n",
    "    # Observation space should be defined here.\n",
    "    # lru_cache allows observation and action spaces to be memoized, reducing clock cycles required to get each agent's space.\n",
    "    # If your spaces change over time, remove this line (disable caching).\n",
    "    @functools.lru_cache(maxsize=None)\n",
    "    def observation_space(self, agent):\n",
    "        return self.env.observation_space(agent)\n",
    "\n",
    "    # Action space should be defined here.\n",
    "    # If your spaces change over time, remove this line (disable caching).\n",
    "    @functools.lru_cache(maxsize=None)\n",
    "    def action_space(self, agent):\n",
    "        return self.env.action_space(agent)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
