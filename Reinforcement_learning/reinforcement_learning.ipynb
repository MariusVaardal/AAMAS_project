{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pettingzoo.mpe import simple_tag_v3\n",
    "from pettingzoo import ParallelEnv\n",
    "import numpy as np\n",
    "import functools\n",
    "import os\n",
    "import sys\n",
    "from utils.utils import PROJECT_PATH\n",
    "sys.path.append(PROJECT_PATH)\n",
    "\n",
    "import supersuit as ss\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold, BaseCallback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_GOOD = 1\n",
    "NUM_ADV = 2\n",
    "NUM_OBST = 0\n",
    "MAX_CYCLES = 200\n",
    "CONTINOUS_ACTIONS = False\n",
    "RENDER_MODE = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_agent_0_from_dicts(dicts):\n",
    "    ret = []\n",
    "    for dict in dicts:\n",
    "        del dict['agent_0']\n",
    "        ret.append(dict)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent_types.AvoidingAgent import AvoidingAgent\n",
    "\n",
    "class CustomEnvironment(ParallelEnv):\n",
    "    metadata = {\n",
    "        \"name\": \"custom_environment_v0\",\n",
    "    }\n",
    "\n",
    "    def __init__(self, num_good, num_adversaries, num_obstacles, max_cycles, continuous_actions, render_mode):\n",
    "        self.env = simple_tag_v3.parallel_env(num_good=num_good, num_adversaries=num_adversaries, num_obstacles=num_obstacles, max_cycles=max_cycles, continuous_actions=continuous_actions, render_mode=render_mode)\n",
    "        self.env.reset() \n",
    "        # Setting all the required attributes\n",
    "        self.agents = [agent for agent in self.env.agents if agent.startswith(\"adversary\")]\n",
    "        self.possible_agents = [adv for adv in self.env.possible_agents if adv.startswith(\"adversary\")]\n",
    "        self.render_mode = render_mode\n",
    "        # Adding agent_0 as part of the environment. Agent_0 is not meant to be included in the training\n",
    "        # self.agent_0 = AvoidingNearestAdversaryAgent('agent_0', num_adversaries=NUM_ADV, num_landmarks=NUM_OBST)\n",
    "        self.agent_0 = AvoidingAgent('agent_0', num_adversaries=num_adversaries, num_landmarks=NUM_OBST)\n",
    "        \n",
    "    def reset(self, seed=None, options=None):\n",
    "        observations, infos = self.env.reset(seed=seed, options=options)\n",
    "        self.agent_0.see(observations[self.agent_0.name])\n",
    "        observations, infos = remove_agent_0_from_dicts([observations, infos])\n",
    "        return observations, infos\n",
    "\n",
    "    def step(self, actions):\n",
    "        actions['agent_0'] = self.agent_0.get_action()\n",
    "        observations, rewards, terminations, truncations, infos =  self.env.step(actions)\n",
    "        if observations:\n",
    "            self.agent_0.see(observations[self.agent_0.name])\n",
    "            observations, rewards, terminations, truncations, infos = remove_agent_0_from_dicts([observations, rewards, terminations, truncations, infos])\n",
    "        return observations, rewards, terminations, truncations, infos\n",
    "\n",
    "    def render(self):\n",
    "        self.env.render()\n",
    "\n",
    "    # Observation space should be defined here.\n",
    "    # lru_cache allows observation and action spaces to be memoized, reducing clock cycles required to get each agent's space.\n",
    "    # If your spaces change over time, remove this line (disable caching).\n",
    "    @functools.lru_cache(maxsize=None)\n",
    "    def observation_space(self, agent):\n",
    "        return self.env.observation_space(agent)\n",
    "\n",
    "    # Action space should be defined here.\n",
    "    # If your spaces change over time, remove this line (disable caching).\n",
    "    @functools.lru_cache(maxsize=None)\n",
    "    def action_space(self, agent):\n",
    "        return self.env.action_space(agent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CustomEnvironment(num_good=NUM_GOOD, num_adversaries=NUM_ADV, num_obstacles=NUM_OBST, max_cycles=MAX_CYCLES, continuous_actions=CONTINOUS_ACTIONS, render_mode=RENDER_MODE)\n",
    "env.reset(seed=45)\n",
    "conv_env = ss.pettingzoo_env_to_vec_env_v1(env)\n",
    "conv_env = ss.concat_vec_envs_v1(conv_env, 2, num_cpus=0, base_class=\"stable_baselines3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_rate_schedule():\n",
    "    def func(progress_remaining):\n",
    "        if progress_remaining > 0.5:\n",
    "            return 0.0001\n",
    "        if progress_remaining > 0.25:\n",
    "            return 0.00001\n",
    "        return 0.000001\n",
    "    return func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "log_path = \"./logs/2_adv_0_to_50M_steps\"\n",
    "    \n",
    "\n",
    "model = PPO(\n",
    "        MlpPolicy,\n",
    "        conv_env,\n",
    "        verbose=3,\n",
    "        learning_rate=learning_rate_schedule(),\n",
    "        batch_size=256,\n",
    "        #tensorboard_log=log_path,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import CallbackList, CheckpointCallback, EvalCallback\n",
    "\n",
    "best_model_save_path = \"./models/2_adv/\"\n",
    "\n",
    "# # Save a checkpoint every 1000 steps\n",
    "checkpoint_callback = CheckpointCallback(\n",
    "                             save_freq=500_000,\n",
    "                             save_path=\"./models/2_adv\",\n",
    "                             name_prefix=\"2_adv_after_15M_steps\",\n",
    "                            )\n",
    "eval_callback = EvalCallback(conv_env,\n",
    "                             eval_freq=10_000,\n",
    "                             best_model_save_path=best_model_save_path,\n",
    "                             log_path=log_path,\n",
    "                             n_eval_episodes=100,\n",
    "                             verbose=1)\n",
    "\n",
    "callback_list = CallbackList([eval_callback, checkpoint_callback])\n",
    "\n",
    "model.learn(total_timesteps=30_000_000, callback=callback_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mariusvaardal/.local/lib/python3.10/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2.32, 7.497839688870388)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "evaluate_policy(model=model, env=conv_env, n_eval_episodes=500)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
